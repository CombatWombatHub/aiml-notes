
<!DOCTYPE html>


<html lang="en" data-content_root="../../" >

  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="viewport" content="width=device-width, initial-scale=1" />

    <title>Natural Language Processing &#8212; aiml-notes 1.0 documentation</title>
  
  
  
  <script data-cfasync="false">
    document.documentElement.dataset.mode = localStorage.getItem("mode") || "";
    document.documentElement.dataset.theme = localStorage.getItem("theme") || "";
  </script>
  
  <!-- Loaded before other Sphinx assets -->
  <link href="../../_static/styles/theme.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />
<link href="../../_static/styles/bootstrap.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />
<link href="../../_static/styles/pydata-sphinx-theme.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />

  
  <link href="../../_static/vendor/fontawesome/6.5.2/css/all.min.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />
  <link rel="preload" as="font" type="font/woff2" crossorigin href="../../_static/vendor/fontawesome/6.5.2/webfonts/fa-solid-900.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="../../_static/vendor/fontawesome/6.5.2/webfonts/fa-brands-400.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="../../_static/vendor/fontawesome/6.5.2/webfonts/fa-regular-400.woff2" />

    <link rel="stylesheet" type="text/css" href="../../_static/pygments.css?v=8f2a1f02" />
    <link rel="stylesheet" type="text/css" href="../../_static/styles/sphinx-book-theme.css?v=eba8b062" />
    <link rel="stylesheet" type="text/css" href="../../_static/mystnb.8ecb98da25f57f5357bf6f572d296f466b2cfe2517ffebfabe82451661e28f02.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/sphinx-design.min.css?v=95c83b7e" />
    <link rel="stylesheet" type="text/css" href="../../_static/custom.css?v=43d83c71" />
  
  <!-- Pre-loaded scripts that we'll load fully later -->
  <link rel="preload" as="script" href="../../_static/scripts/bootstrap.js?digest=dfe6caa3a7d634c4db9b" />
<link rel="preload" as="script" href="../../_static/scripts/pydata-sphinx-theme.js?digest=dfe6caa3a7d634c4db9b" />
  <script src="../../_static/vendor/fontawesome/6.5.2/js/all.min.js?digest=dfe6caa3a7d634c4db9b"></script>

    <script src="../../_static/documentation_options.js?v=a681ed88"></script>
    <script src="../../_static/doctools.js?v=9bcbadda"></script>
    <script src="../../_static/sphinx_highlight.js?v=dc90522c"></script>
    <script src="../../_static/scripts/sphinx-book-theme.js?v=887ef09a"></script>
    <script src="../../_static/design-tabs.js?v=f930bc37"></script>
    <script type="module" src="https://cdn.jsdelivr.net/npm/mermaid@11.2.0/dist/mermaid.esm.min.mjs"></script>
    <script type="module" src="https://cdn.jsdelivr.net/npm/@mermaid-js/layout-elk@0.1.4/dist/mermaid-layout-elk.esm.min.mjs"></script>
    <script type="module">import mermaid from "https://cdn.jsdelivr.net/npm/mermaid@11.2.0/dist/mermaid.esm.min.mjs";import elkLayouts from "https://cdn.jsdelivr.net/npm/@mermaid-js/layout-elk@0.1.4/dist/mermaid-layout-elk.esm.min.mjs";mermaid.registerLayoutLoaders(elkLayouts);mermaid.initialize({startOnLoad:false});</script>
    <script src="https://cdn.jsdelivr.net/npm/d3@7.9.0/dist/d3.min.js"></script>
    <script type="module">
import mermaid from "https://cdn.jsdelivr.net/npm/mermaid@11.2.0/dist/mermaid.esm.min.mjs";
window.addEventListener("load", () => mermaid.run());
</script>
    <script>DOCUMENTATION_OPTIONS.pagename = 'ai/categories/nlp';</script>
    <link rel="icon" href="../../_static/favicon.ico"/>
    <link rel="index" title="Index" href="../../genindex.html" />
    <link rel="search" title="Search" href="../../search.html" />
    <link rel="next" title="Development" href="../development/index.html" />
    <link rel="prev" title="Machine Learning" href="ml.html" />
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <meta name="docsearch:language" content="en"/>
  </head>
  
  
  <body data-bs-spy="scroll" data-bs-target=".bd-toc-nav" data-offset="180" data-bs-root-margin="0px 0px -60%" data-default-mode="">

  
  
  <div id="pst-skip-link" class="skip-link d-print-none"><a href="#main-content">Skip to main content</a></div>
  
  <div id="pst-scroll-pixel-helper"></div>
  
  <button type="button" class="btn rounded-pill" id="pst-back-to-top">
    <i class="fa-solid fa-arrow-up"></i>Back to top</button>

  
  <input type="checkbox"
          class="sidebar-toggle"
          id="pst-primary-sidebar-checkbox"/>
  <label class="overlay overlay-primary" for="pst-primary-sidebar-checkbox"></label>
  
  <input type="checkbox"
          class="sidebar-toggle"
          id="pst-secondary-sidebar-checkbox"/>
  <label class="overlay overlay-secondary" for="pst-secondary-sidebar-checkbox"></label>
  
  <div class="search-button__wrapper">
    <div class="search-button__overlay"></div>
    <div class="search-button__search-container">
<form class="bd-search d-flex align-items-center"
      action="../../search.html"
      method="get">
  <i class="fa-solid fa-magnifying-glass"></i>
  <input type="search"
         class="form-control"
         name="q"
         id="search-input"
         placeholder="Search..."
         aria-label="Search..."
         autocomplete="off"
         autocorrect="off"
         autocapitalize="off"
         spellcheck="false"/>
  <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd>K</kbd></span>
</form></div>
  </div>

  <div class="pst-async-banner-revealer d-none">
  <aside id="bd-header-version-warning" class="d-none d-print-none" aria-label="Version warning"></aside>
</div>

  
    <header class="bd-header navbar navbar-expand-lg bd-navbar d-print-none">
    </header>
  

  <div class="bd-container">
    <div class="bd-container__inner bd-page-width">
      
      
      
      <div class="bd-sidebar-primary bd-sidebar">
        

  
  <div class="sidebar-header-items sidebar-primary__section">
    
    
    
    
  </div>
  
    <div class="sidebar-primary-items__start sidebar-primary__section">
        <div class="sidebar-primary-item">

  
    
  

<a class="navbar-brand logo" href="../../index.html">
  
  
  
  
  
    
    
      
    
    
    <img src="../../_static/logo.jpg" class="logo__image only-light" alt="aiml-notes 1.0 documentation - Home"/>
    <script>document.write(`<img src="../../_static/logo.jpg" class="logo__image only-dark" alt="aiml-notes 1.0 documentation - Home"/>`);</script>
  
  
</a></div>
        <div class="sidebar-primary-item">

 <script>
 document.write(`
   <button class="btn search-button-field search-button__button" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="fa-solid fa-magnifying-glass"></i>
    <span class="search-button__default-text">Search</span>
    <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd class="kbd-shortcut__modifier">K</kbd></span>
   </button>
 `);
 </script></div>
        <div class="sidebar-primary-item"><nav class="bd-links bd-docs-nav" aria-label="Main">
    <div class="bd-toc-item navbar-nav active">
        
        <ul class="nav bd-sidenav bd-sidenav__home-link">
            <li class="toctree-l1">
                <a class="reference internal" href="../../index.html">
                    AI/ML Notes
                </a>
            </li>
        </ul>
        <p aria-level="2" class="caption" role="heading"><span class="caption-text">Notes:</span></p>
<ul class="current nav bd-sidenav">
<li class="toctree-l1 current active has-children"><a class="reference internal" href="index.html">Categories</a><details open="open"><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul class="current">
<li class="toctree-l2"><a class="reference internal" href="generative.html">Generative AI</a></li>
<li class="toctree-l2"><a class="reference internal" href="ml.html">Machine Learning</a></li>
<li class="toctree-l2 current active"><a class="current reference internal" href="#">Natural Language Processing</a></li>
</ul>
</details></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../development/index.html">Development</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="../development/developing.html">ML Development</a></li>
<li class="toctree-l2"><a class="reference internal" href="../development/fundamentals.html">Fundamentals</a></li>
<li class="toctree-l2"><a class="reference internal" href="../development/metrics.html">Metrics</a></li>
<li class="toctree-l2"><a class="reference internal" href="../development/resources.html">Resources</a></li>
</ul>
</details></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Portfolio:</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../../portfolio/portfolio.html">Portfolio</a></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../../portfolio/kaggle/index.html">Kaggle</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="../../portfolio/kaggle/metrics.html">Metrics and correlation</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../portfolio/kaggle/nlp_beginners_guide_noex.html">Getting Started with NLP</a></li>
</ul>
</details></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../../portfolio/machine_learning_mastery/index.html">Machine Learning Mastery</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="../../portfolio/machine_learning_mastery/python_ml_mini_course/python_ml_mini_course.html">Python Mini Course</a></li>
</ul>
</details></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../../portfolio/datacamp/index.html">DataCamp</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="../../portfolio/datacamp/supervised_learning_scikit_learn/supervised_learning_scikit_learn.html">Supervised Learning</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../portfolio/datacamp/using_datacamp.html">Using DataCamp</a></li>
</ul>
</details></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Sphinx:</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1 has-children"><a class="reference internal" href="../../sphinx_examples/index.html">Sphinx</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="../../sphinx_examples/markdown.html">Markdown</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../sphinx_examples/notebook.html">Jupyter Notebook</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../sphinx_examples/restructuredtext.html">reStructuredText</a></li>
</ul>
</details></li>
</ul>

    </div>
</nav></div>
    </div>
  
  
  <div class="sidebar-primary-items__end sidebar-primary__section">
  </div>
  
  <div id="rtd-footer-container"></div>


      </div>
      
      <main id="main-content" class="bd-main" role="main">
        
        

<div class="sbt-scroll-pixel-helper"></div>

          <div class="bd-content">
            <div class="bd-article-container">
              
              <div class="bd-header-article d-print-none">
<div class="header-article-items header-article__inner">
  
    <div class="header-article-items__start">
      
        <div class="header-article-item"><button class="sidebar-toggle primary-toggle btn btn-sm" title="Toggle primary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
  <span class="fa-solid fa-bars"></span>
</button></div>
      
    </div>
  
  
    <div class="header-article-items__end">
      
        <div class="header-article-item">

<div class="article-header-buttons">


<a href="https://github.com/CombatWombatHub/aiml-notes" target="_blank"
   class="btn btn-sm btn-source-repository-button"
   title="Source repository"
   data-bs-placement="bottom" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fab fa-github"></i>
  </span>

</a>






<div class="dropdown dropdown-download-buttons">
  <button class="btn dropdown-toggle" type="button" data-bs-toggle="dropdown" aria-expanded="false" aria-label="Download this page">
    <i class="fas fa-download"></i>
  </button>
  <ul class="dropdown-menu">
      
      
      
      <li><a href="../../_sources/ai/categories/nlp.md" target="_blank"
   class="btn btn-sm btn-download-source-button dropdown-item"
   title="Download source file"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-file"></i>
  </span>
<span class="btn__text-container">.md</span>
</a>
</li>
      
      
      
      
      <li>
<button onclick="window.print()"
  class="btn btn-sm btn-download-pdf-button dropdown-item"
  title="Print to PDF"
  data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-file-pdf"></i>
  </span>
<span class="btn__text-container">.pdf</span>
</button>
</li>
      
  </ul>
</div>




<button onclick="toggleFullScreen()"
  class="btn btn-sm btn-fullscreen-button"
  title="Fullscreen mode"
  data-bs-placement="bottom" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-expand"></i>
  </span>

</button>



<script>
document.write(`
  <button class="btn btn-sm nav-link pst-navbar-icon theme-switch-button" title="light/dark" aria-label="light/dark" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="theme-switch fa-solid fa-sun fa-lg" data-mode="light"></i>
    <i class="theme-switch fa-solid fa-moon fa-lg" data-mode="dark"></i>
    <i class="theme-switch fa-solid fa-circle-half-stroke fa-lg" data-mode="auto"></i>
  </button>
`);
</script>


<script>
document.write(`
  <button class="btn btn-sm pst-navbar-icon search-button search-button__button" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="fa-solid fa-magnifying-glass fa-lg"></i>
  </button>
`);
</script>
<button class="sidebar-toggle secondary-toggle btn btn-sm" title="Toggle secondary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <span class="fa-solid fa-list"></span>
</button>
</div></div>
      
    </div>
  
</div>
</div>
              
              

<div id="jb-print-docs-body" class="onlyprint">
    <h1>Natural Language Processing</h1>
    <!-- Table of contents -->
    <div id="print-main-content">
        <div id="jb-print-toc">
            
            <div>
                <h2> Contents </h2>
            </div>
            <nav aria-label="Page">
                <ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#large-language-models-llm-s">Large Language Models (LLM’s)</a><ul class="visible nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#large-language-models-explained-briefly">Large Language Models Explained Briefly</a><ul class="visible nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#prompting">Prompting</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#pretraining">Pretraining</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#reinforcement-learning-with-human-feedback-rlhf">Reinforcement Learning with Human Feedback (RLHF)</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#transformers">Transformers</a><ul class="visible nav section-nav flex-column">
<li class="toc-h5 nav-item toc-entry"><a class="reference internal nav-link" href="#attention">Attention</a></li>
<li class="toc-h5 nav-item toc-entry"><a class="reference internal nav-link" href="#feedforward">Feedforward</a><ul class="nav section-nav flex-column">
<li class="toc-h6 nav-item toc-entry"><a class="reference internal nav-link" href="#repeat">Repeat</a></li>
</ul>
</li>
<li class="toc-h5 nav-item toc-entry"><a class="reference internal nav-link" href="#final-vector">Final Vector</a></li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#hugging-face-transformers-library">Hugging Face Transformers Library</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#maintenance-records-classification-task">Maintenance Records Classification Task</a></li>
</ul>
            </nav>
        </div>
    </div>
</div>

              
                
<div id="searchbox"></div>
                <article class="bd-article">
                  
  <section class="tex2jax_ignore mathjax_ignore" id="natural-language-processing">
<h1>Natural Language Processing<a class="headerlink" href="#natural-language-processing" title="Link to this heading">#</a></h1>
<p><a class="reference external" href="https://www.geeksforgeeks.org/nlp/natural-language-processing-nlp-tutorial/">Natural Language Processing (NLP)</a> understands and interacts with human languages in a way that feels natural. <code class="docutils literal notranslate"><span class="pre">NLP</span></code> uses <a class="reference internal" href="ml.html"><span class="std std-doc">ML</span></a>, but isn’t <a class="reference internal" href="ml.html"><span class="std std-doc">ML</span></a> itself. <code class="docutils literal notranslate"><span class="pre">NLP</span></code> is the application, <a class="reference internal" href="ml.html"><span class="std std-doc">ML</span></a> is the engine.</p>
<ul class="simple">
<li><p><a class="reference external" href="https://www.kaggle.com/learn-guide/natural-language-processing">Kaggle Natural Language Processing Guide</a> has links to a lot of relevant tutorials and project ideas. The best place to start is probably their guide on <a class="reference external" href="https://www.kaggle.com/code/jhoward/getting-started-with-nlp-for-absolute-beginners">Getting started with NLP for absolute beginners</a></p></li>
</ul>
<section id="large-language-models-llm-s">
<h2>Large Language Models (LLM’s)<a class="headerlink" href="#large-language-models-llm-s" title="Link to this heading">#</a></h2>
<p>The best videos to summarize these are from the 3Blue1Brown <a class="reference external" href="https://youtube.com/playlist?list=PLZHQObOWTQDNU6R1_67000Dx_ZCJB-3pi&amp;amp;si=0_HYbGimQB-Tx1tF">Deep Learning / Neural Networks Course</a> - 8 videos covering neural networks, backpropagation, LLM’s, and Transformers. Each video is 8-30 minutes long, total runtime about 2 hours. The videos on Neural Networks were made way back in 2018, and the LLM videos were made in 2024, so there’s a chance more will be added if something else game-changing is discovered. For an overview, just watch the 8 minute video <a class="reference external" href="https://youtu.be/LPZh9BOjkQs?si=K21ni-7Kl-A28Te0">Large language models explained briefly</a>.</p>
<ul class="simple">
<li><p>Neural Networks</p>
<ul>
<li><p><a class="reference external" href="https://youtu.be/aircAruvnKk?si=HxQewJQ3Zz8GRZYL">What is a neural network?</a></p></li>
<li><p><a class="reference external" href="https://youtu.be/IHZwWFHWa-w?si=fgadmhmJntK_--Ox">Gradient descent, how neural networks learn</a></p></li>
<li><p><a class="reference external" href="https://youtu.be/Ilg3gGewQ5U?si=MCDaQJPucfo5qGq2">Backpropagation, intuitively</a></p></li>
<li><p><a class="reference external" href="https://youtu.be/tIeHLnjs5U8?si=akKnuCFaTsUaXm1y">Backpropagation, calculus</a></p></li>
</ul>
</li>
<li><p>LLM’s</p>
<ul>
<li><p><a class="reference external" href="https://youtu.be/LPZh9BOjkQs?si=K21ni-7Kl-A28Te0">Large language models explained briefly</a></p></li>
<li><p><a class="reference external" href="https://youtu.be/wjZofJX0v4M?si=e6bkNOMBgaE27dcx">Transformers, the tech behind LLM’s</a></p></li>
<li><p><a class="reference external" href="https://youtu.be/eMlx5fFNoYc?si=J5E4BdXGkBsotegN">Attention in transformers</a></p></li>
<li><p><a class="reference external" href="https://youtu.be/9-Jl0dxWQs8?si=nyvCnrZJ19H1L7Vu">How LLM’s store facts</a></p></li>
</ul>
</li>
</ul>
<section id="large-language-models-explained-briefly">
<h3><a class="reference external" href="https://youtu.be/LPZh9BOjkQs?si=K21ni-7Kl-A28Te0">Large Language Models Explained Briefly</a><a class="headerlink" href="#large-language-models-explained-briefly" title="Link to this heading">#</a></h3>
<p>An <strong>LLM</strong> is a sophisticated mathematical function that predicts what word comes next for any given text.
The output is a <em>probability</em> for each possible token (tokens are often words, but might contain punctuation, etc).
You choose one of the <strong>likely</strong> tokens from the list, but not always the <strong>most likely</strong> in order to make the response more natural (I believe that the “temperature” setting controls how far the choices diverge from the most likely).</p>
<p>You do this over and over, feeding in the initial prompt as well as all predicted tokens, predicting one more token at a time until you’ve got a full response.</p>
<section id="prompting">
<h4>Prompting<a class="headerlink" href="#prompting" title="Link to this heading">#</a></h4>
<p>Since it’s just predicting one of the likely words over and over, you need to set up the initial prompt so that the likely words are the ones that will best answer the question.</p>
<p>So if a user asked what to do on their vacation, you might add the following so that the likely words are also the most helpful (AKA prompt engineering):</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">What</span> <span class="n">follows</span> <span class="ow">is</span> <span class="n">a</span> <span class="n">conversation</span> <span class="n">between</span> <span class="n">a</span> <span class="n">user</span> <span class="ow">and</span> <span class="n">a</span> <span class="n">helpful</span><span class="p">,</span> <span class="n">very</span> <span class="n">knowledgeable</span> <span class="n">AI</span> <span class="n">assistant</span>

<span class="n">Use</span><span class="p">:</span> <span class="n">Give</span> <span class="n">me</span> <span class="n">some</span> <span class="n">ideas</span> <span class="k">for</span> <span class="n">what</span> <span class="n">to</span> <span class="n">do</span> <span class="n">when</span> <span class="n">visiting</span> <span class="n">Santiago</span><span class="o">.</span>

<span class="n">AI</span> <span class="n">Assistant</span><span class="p">:</span> <span class="n">______</span>
</pre></div>
</div>
</section>
<section id="pretraining">
<h4>Pretraining<a class="headerlink" href="#pretraining" title="Link to this heading">#</a></h4>
<p>To be able to predict the likely tokens like a human would, the model needs to be trained by feeding it human-written text.
A <strong>massive</strong> amount of it.
Since you need so much, you normally get it from the internet.</p>
<p>Each training sample can be a handful of words (like a text message) or thousands (like a novel).
You pass the model all but the last token of a string of text, then compare the prediction it makes with the true last token that you withheld.
After that, you tweak the model to make it more likely that the model will predict the true token and less likely to predict the other tokens.</p>
<p>The model is adjusted by changing the (potentially <strong>billions</strong>) of <code class="docutils literal notranslate"><span class="pre">parameters</span></code>/<code class="docutils literal notranslate"><span class="pre">weights</span></code> through a process known as <code class="docutils literal notranslate"><span class="pre">backpropagation</span></code>.
The staggering number of these weights is what puts the <strong>L</strong> in <strong>LLMs</strong>.
The graphic he uses for an LLM looks like a standard neural network, with each layer being a 2D matrix.
The scale of operations required to train the larger LLMs is astronomical.</p>
<p>On top of that, this <code class="docutils literal notranslate"><span class="pre">pretraining</span></code> step is only the first part of the process.
What you’ve got after this step is an AI that’s very good at auto-completing random text from the internet.
That’s a far cry from having a helpful AI assistant.</p>
</section>
<section id="reinforcement-learning-with-human-feedback-rlhf">
<h4>Reinforcement Learning with Human Feedback (RLHF)<a class="headerlink" href="#reinforcement-learning-with-human-feedback-rlhf" title="Link to this heading">#</a></h4>
<p>A huge number of human workers flag problematic/unhelpful predictions, which further tweaks the weights of the model to prefer to predict helpful responses.</p>
</section>
<section id="transformers">
<h4>Transformers<a class="headerlink" href="#transformers" title="Link to this heading">#</a></h4>
<p>The process is only possible through the use of <strong>Graphical Processing Units (GPUs)</strong>, which can perform operations in parallel, <strong>BUT</strong> prior to 2017, not all language operations could be easily parallelized, and models needed to process one word at a time (i.e. each time you needed to predict a new word, it needed to process each word of the prompt and all predictions so far one word at a time).</p>
<p>Google introduced a new model - the <code class="docutils literal notranslate"><span class="pre">Transformer</span></code> - which doesn’t need to read the input text from the start to the finish. They read it all in at once, in parallel.</p>
<p>Each token in the prompt is associated with a <code class="docutils literal notranslate"><span class="pre">vector</span></code> - a long list of numbers (since neural networks work off of continuous numbers), which encode the <strong>meaning</strong> of the token.</p>
<section id="attention">
<h5>Attention<a class="headerlink" href="#attention" title="Link to this heading">#</a></h5>
<p><code class="docutils literal notranslate"><span class="pre">Transformers</span></code> are associated with a special operation known as <code class="docutils literal notranslate"><span class="pre">Attention</span></code>, which gives the <code class="docutils literal notranslate"><span class="pre">vectors</span></code> of each <code class="docutils literal notranslate"><span class="pre">token</span></code> in the prompt a chance to talk with one another and figure out which ones work together. It uses the context of surrounding <code class="docutils literal notranslate"><span class="pre">tokens</span></code> to inform the more specific meaning of other <code class="docutils literal notranslate"><span class="pre">tokens</span></code>, all in parallel.</p>
<p>For instance, the <code class="docutils literal notranslate"><span class="pre">token</span></code> for “<code class="docutils literal notranslate"><span class="pre">bank</span></code>” in this sentence has its <code class="docutils literal notranslate"><span class="pre">vector</span></code> modified by the <code class="docutils literal notranslate"><span class="pre">vectors</span></code> for the <code class="docutils literal notranslate"><span class="pre">tokens</span></code> <code class="docutils literal notranslate"><span class="pre">deposit</span></code> and <code class="docutils literal notranslate"><span class="pre">check</span></code> to encode the meaning of a <code class="docutils literal notranslate"><span class="pre">financial</span> <span class="pre">bank</span></code></p>
<pre  class="mermaid">
        flowchart LR
1[deposit]
2[a]
3[check]
4[at]
5[the]
6[bank]

1 ~~~ 2 ~~~ 3 ~~~ 4 ~~~ 5 ~~~ 6
1 &amp; 3 --&gt; 6
    </pre><p>While the same <code class="docutils literal notranslate"><span class="pre">token</span></code> “<code class="docutils literal notranslate"><span class="pre">bank</span></code>” in this sentence would have its <code class="docutils literal notranslate"><span class="pre">vector</span></code> modified by “<code class="docutils literal notranslate"><span class="pre">river</span></code>” so that its meaning is encoded as a <code class="docutils literal notranslate"><span class="pre">river</span> <span class="pre">bank</span></code>.</p>
<pre  class="mermaid">
        flowchart LR
7[Down]
8[by]
9[the]
10[river]
11[bank]

7 ~~~ 8 ~~~ 9 ~~~ 10 ~~~ 11
10 --&gt; 11
    </pre><p>I’m guessing the token’s vector doesn’t get modified in-place, but rather each of these layers outputs a vector for each token which is modified for the operation (otherwise I can’t think of how <code class="docutils literal notranslate"><span class="pre">backpropagation</span></code> would work)</p>
</section>
<section id="feedforward">
<h5>Feedforward<a class="headerlink" href="#feedforward" title="Link to this heading">#</a></h5>
<p><code class="docutils literal notranslate"><span class="pre">Transformers</span></code> typically also include a <code class="docutils literal notranslate"><span class="pre">Feedforward</span> <span class="pre">Neural</span> <span class="pre">Network</span></code> operation, which gives extra capacity to store more information about patterns during training.</p>
<section id="repeat">
<h6>Repeat<a class="headerlink" href="#repeat" title="Link to this heading">#</a></h6>
<p>The <code class="docutils literal notranslate"><span class="pre">transformer</span></code> will generally have multiple iterations of these two operations - an <code class="docutils literal notranslate"><span class="pre">Attention</span></code>operation followed by a <code class="docutils literal notranslate"><span class="pre">Feedforward</span></code> operation followed by an <code class="docutils literal notranslate"><span class="pre">Attention</span></code> operation and so on. Each successive pair should further encode the meaning of each <code class="docutils literal notranslate"><span class="pre">token</span></code> to make a more accurate prediction about the last word.</p>
<p>I’m guessing that the <code class="docutils literal notranslate"><span class="pre">attention</span></code> and <code class="docutils literal notranslate"><span class="pre">feedforward</span></code> blocks are also trained during <code class="docutils literal notranslate"><span class="pre">pretraining</span></code> and <code class="docutils literal notranslate"><span class="pre">RLHF</span></code></p>
</section>
</section>
<section id="final-vector">
<h5>Final Vector<a class="headerlink" href="#final-vector" title="Link to this heading">#</a></h5>
<p>The final vector in the sequence, once influenced by everything the model learned during training as well as the context encoded by the <code class="docutils literal notranslate"><span class="pre">attention</span></code> and <code class="docutils literal notranslate"><span class="pre">feedforward</span></code> stacks, will do one final operation. it will output a vector with one probability for each possible <code class="docutils literal notranslate"><span class="pre">token</span></code> that could come next.</p>
</section>
</section>
</section>
</section>
<section id="hugging-face-transformers-library">
<h2>Hugging Face Transformers Library<a class="headerlink" href="#hugging-face-transformers-library" title="Link to this heading">#</a></h2>
<ul class="simple">
<li><p>the Kaggle <a class="reference external" href="https://www.kaggle.com/code/jhoward/getting-started-with-nlp-for-absolute-beginners">Getting started with NLP for absolute beginners</a> uses the <code class="docutils literal notranslate"><span class="pre">transformers</span></code> Python package from <a class="reference external" href="https://huggingface.co/docs/transformers/en/index">Hugging Face</a></p></li>
<li><p>it appears to be a library specifically to help use <code class="docutils literal notranslate"><span class="pre">Transformers</span></code> models</p></li>
<li><p>sounds like it helps you download and use pretrained models (since you wouldn’t want to try to train an LLM yourself unless you own a mountain lair full of GPU’s)</p></li>
<li><p>also lets you attach stuff to do the other parts of the process like tokenization</p></li>
</ul>
</section>
<section id="maintenance-records-classification-task">
<h2>Maintenance Records Classification Task<a class="headerlink" href="#maintenance-records-classification-task" title="Link to this heading">#</a></h2>
<ul class="simple">
<li><p>Trying to remember and practice a particular problem</p></li>
<li><p>dataset was perhaps four columns, I sort of remember 3 of them</p></li>
<li><p>need to predict the category from the other data</p></li>
<li><p>first order of business: find a similar dataset or problem</p></li>
<li><p>first place to start is probably the Kaggle Guide <a class="reference external" href="https://www.kaggle.com/code/jhoward/getting-started-with-nlp-for-absolute-beginners">Getting started with NLP for absolute beginners</a>.</p></li>
</ul>
<div class="pst-scrollable-table-container"><table class="table">
<thead>
<tr class="row-odd"><th class="head"><p>date</p></th>
<th class="head"><p>service</p></th>
<th class="head"><p>category</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p>4/13/2004</p></td>
<td><p>patched hull</p></td>
<td><p>watercraft</p></td>
</tr>
<tr class="row-odd"><td><p>6/1/2005</p></td>
<td><p>tire change</p></td>
<td><p>vehicle</p></td>
</tr>
<tr class="row-even"><td><p>6/22/2005</p></td>
<td><p>waxed wing, swapped landing gear spring</p></td>
<td><p>aircraft</p></td>
</tr>
<tr class="row-odd"><td><p>…</p></td>
<td><p>…</p></td>
<td><p>…</p></td>
</tr>
</tbody>
</table>
</div>
</section>
</section>


                </article>
              

              
              
              
              
                <footer class="prev-next-footer d-print-none">
                  
<div class="prev-next-area">
    <a class="left-prev"
       href="ml.html"
       title="previous page">
      <i class="fa-solid fa-angle-left"></i>
      <div class="prev-next-info">
        <p class="prev-next-subtitle">previous</p>
        <p class="prev-next-title">Machine Learning</p>
      </div>
    </a>
    <a class="right-next"
       href="../development/index.html"
       title="next page">
      <div class="prev-next-info">
        <p class="prev-next-subtitle">next</p>
        <p class="prev-next-title">Development</p>
      </div>
      <i class="fa-solid fa-angle-right"></i>
    </a>
</div>
                </footer>
              
            </div>
            
            
              
                <div class="bd-sidebar-secondary bd-toc"><div class="sidebar-secondary-items sidebar-secondary__inner">


  <div class="sidebar-secondary-item">
  <div class="page-toc tocsection onthispage">
    <i class="fa-solid fa-list"></i> Contents
  </div>
  <nav class="bd-toc-nav page-toc">
    <ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#large-language-models-llm-s">Large Language Models (LLM’s)</a><ul class="visible nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#large-language-models-explained-briefly">Large Language Models Explained Briefly</a><ul class="visible nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#prompting">Prompting</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#pretraining">Pretraining</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#reinforcement-learning-with-human-feedback-rlhf">Reinforcement Learning with Human Feedback (RLHF)</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#transformers">Transformers</a><ul class="visible nav section-nav flex-column">
<li class="toc-h5 nav-item toc-entry"><a class="reference internal nav-link" href="#attention">Attention</a></li>
<li class="toc-h5 nav-item toc-entry"><a class="reference internal nav-link" href="#feedforward">Feedforward</a><ul class="nav section-nav flex-column">
<li class="toc-h6 nav-item toc-entry"><a class="reference internal nav-link" href="#repeat">Repeat</a></li>
</ul>
</li>
<li class="toc-h5 nav-item toc-entry"><a class="reference internal nav-link" href="#final-vector">Final Vector</a></li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#hugging-face-transformers-library">Hugging Face Transformers Library</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#maintenance-records-classification-task">Maintenance Records Classification Task</a></li>
</ul>
  </nav></div>

</div></div>
              
            
          </div>
          <footer class="bd-footer-content">
            
<div class="bd-footer-content__inner container">
  
  <div class="footer-item">
    
<p class="component-author">
By Matthew T Gill
</p>

  </div>
  
  <div class="footer-item">
    

  <p class="copyright">
    
      © Copyright 2025.
      <br/>
    
  </p>

  </div>
  
  <div class="footer-item">
    
  </div>
  
  <div class="footer-item">
    
  </div>
  
</div>
          </footer>
        

      </main>
    </div>
  </div>
  
  <!-- Scripts loaded after <body> so the DOM is not blocked -->
  <script src="../../_static/scripts/bootstrap.js?digest=dfe6caa3a7d634c4db9b"></script>
<script src="../../_static/scripts/pydata-sphinx-theme.js?digest=dfe6caa3a7d634c4db9b"></script>

  <footer class="bd-footer">
  </footer>
  </body>
</html>