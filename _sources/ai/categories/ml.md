# Machine Learning
[Machine Learning](https://www.geeksforgeeks.org/machine-learning/machine-learning/) is a branch of Artificial Intelligence that focuses on *models* and [algorithms](https://www.geeksforgeeks.org/machine-learning/machine-learning-algorithms/) that let computers learn from data and improve from previous experience without being explicitly programmed. There are many [types](https://www.geeksforgeeks.org/machine-learning/types-of-machine-learning/) of machine learning.

- [Supervised Learning](https://www.geeksforgeeks.org/machine-learning/supervised-machine-learning/) - Use labeled data
    - [Classification](https://www.geeksforgeeks.org/machine-learning/getting-started-with-classification/) - Predict *categorical* (discrete) values
        - Linear Classifiers
            - [Logistic Regression](https://www.geeksforgeeks.org/machine-learning/understanding-logistic-regression/) - Draws a sigmoid curve, predicts 0 or 1 if above or below curve. Despite "Regression" being in the name, it's for Classification
        - Non-Linear Classifiers
            - [KNN (K-Nearest Neighbors)](https://www.geeksforgeeks.org/machine-learning/k-nearest-neighbours/) ([Notebook](../../datacamp/1_supervised_learning_scikit_learn/1_classification.ipynb))
            - Naive Bayes
        - Both - can be used for Linear and Non-Linear Classifiers
            - [SVM (Support Vector Machine)](https://www.geeksforgeeks.org/machine-learning/support-vector-machine-algorithm/)
    - [Regression](https://www.geeksforgeeks.org/machine-learning/regression-in-machine-learning/) - Predict continuous numerical values
        - [Linear Regression](https://www.geeksforgeeks.org/machine-learning/ml-linear-regression/) - fit a straight line to the data with [Least Squares Method](https://www.geeksforgeeks.org/maths/least-square-method/)
        - Polynomial Regression
        - Ridge Regression
        - Lasso Regression
    - Both - can be used for Regression and Classification
        - Building Blocks - these models are often used as building blocks for Ensemble Methods
        - [Decision Trees](https://www.geeksforgeeks.org/machine-learning/decision-tree-algorithms/) - splits data into branches based on feature values like a flowchart. Often used as building blocks for Ensemble methods.
            - [Decision Tree](https://www.geeksforgeeks.org/machine-learning/decision-tree-introduction-example/) ([Classification](https://www.geeksforgeeks.org/machine-learning/building-and-implementing-decision-tree-classifiers-with-scikit-learn-a-comprehensive-guide/)) ([Regression](https://www.geeksforgeeks.org/machine-learning/python-decision-tree-regression-using-sklearn/)) 
            - [ID3 (Iterative Dichotomiser 3)](https://www.geeksforgeeks.org/machine-learning/iterative-dichotomiser-3-id3-algorithm-from-scratch/)
            - [CART (Classification and Regression Trees)](https://www.geeksforgeeks.org/machine-learning/cart-classification-and-regression-tree-in-machine-learning/)
        - Ensemble Learning - Combine multiple simple models into one better model
            - Bagging (Bootstrap Aggregating) Method - Train models independently on different subsets of the data, then combine their predictions
                - Random Forest ([Regression](https://www.geeksforgeeks.org/machine-learning/random-forest-regression-in-python/))
                - Random Subspace Method
            - Boosting Method - Train models sequentially, each model focusing on errors of prior models, then do weighted combination of their predictions
                - Adaptive Boosting (AdaBoost)
                - Gradient Boosting
                - Extreme Gradient Boosting (XGBoost)
                - CatBoost
            - [Stacking (Stacked Generalization) Method](https://machinelearningmastery.com/implementing-stacking-scratch-python/) - train multiple different models (often different types), use predictions as inputs to final "meta-model"
- [Unsupervised Learning](https://www.geeksforgeeks.org/machine-learning/unsupervised-learning/) - Use unlabeled data
    - Clustering - Group data into clusters based on similarity
        - Centroid-Based Methods
            - K-Means Clustering
            - Elbow Method for optimal value of k in KMeans
            - K-Means++ Clustering
            - K-Mode Clustering
            - FCM (Fuzzy C-Means Clustering)
        - Distribution-Based Methods
            - Gaussian Mixture Models
            - Expectation-Maximization Algorithm
            - DPMMs Dirichlet Process Mixture Models
        - Connectivity-Based Methods
            - Hierarchical Clustering
            - Agglomerative Clustering
            - Divisive Clustering
            - Affinity propagation
        - Density-Based Methods
            - Mean-Shift
            - DBSCAN (Density-Based Spatial Clustering of Applications with Noise)
            - OPTICS (Ordering Points To Identify the Clustering Structure)
    - Dimensionality Reduction - Simplify datasets by reducing features while keeping important information (often used to select features for other models)
        - PCA (Principal Component Analysis)
        - ICA (Independent Component Analysis)
        - t-SNE (t-distributed Stochastic Neighbor Embedding)
        - NMF (Non-negative Matrix Factorization)
        - Isomap
        - LLE (Locally Linear Embedding)
    - Association Rule - Discover rules where the presence of one item in a dataset indicates the probability of the presence of another
        - Apriori
        - ECLAT (Equivalence Class Clustering and Bottom-Up Lattice Traversal)
        - FP-Growth (Frequent Pattern Growth)
- [Reinforcement Learning](https://www.geeksforgeeks.org/machine-learning/what-is-reinforcement-learning/) - Learn from rewards by interacting with environment via trial and error
    - Model-Based Methods - Interact with a simulated environment
        - MDPs (Markov Decision Processes)
        - Bellman Equation
        - Value Iteration Algorithm
        - Monte Carlo Tree Search
    - Model-Free Methods - Interact with the real environment
        - Q-Learning
        - Deep Q-Learning
        - SARSA (State-Action-Reward-State-Action)
        - Monte Carlo Methods
        - Reinforce Algorithm
        - Actor-Critic Algorithm
        - A3C (Asynchronous Advantage Actor-Critic)
- Forecasting Models - Use past data to predict future trends (often time series problems)
    - ARIMA (Auto-Regressive Integrated Moving Average)
    - SARIMA (Seasonal ARIMA)
    - Holt-Winters Exponential Smoothing
- [Semi-Supervised Learning](https://www.geeksforgeeks.org/machine-learning/ml-semi-supervised-learning/) - Use some labeled data with more unlabeled data
    - Graph-Based Semi-Supervised Learning
    - Label Propagation
    - Co-Training
    - Self-Training
    - GANs (Generative Adversarial Networks)
- [Self-Supervised Learning](https://www.geeksforgeeks.org/machine-learning/self-supervised-learning-ssl/) - Generates its own labels from unlabeled data