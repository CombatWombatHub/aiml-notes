{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "20c10f94",
   "metadata": {},
   "source": [
    "# Tokenization\n",
    "- You'd think this was isolated to [Natural Language Processing](../categories/nlp.md), but no.\n",
    "- if you want to use an [Unsupervised](../categories/ml.md) model to cluster documents into categories, you need to tokenize the words there as well\n",
    "- tokenization is breaking strings into chunks (tokens). They're often words, but sometimes not - you  might include the period after a word to capture the fact that that word means something different because it comes at the end of a sentence, or you might grab the `ing` from the end of a word indicating it's an ongoing action.\n",
    "\n",
    "(tf-idf)=\n",
    "## TF-IDF (Term Frequency - Inverse Document Frequency) Matrix\n",
    "- **TF-IDF** is a weighted measure of how important a word is to a document in a collection. It can be used to make a matrix. \n",
    "- **TF (Term Frequency)** - how often a specific word appears in a single document, filtering out words that are probably just one-offs or from a bibliography or something\n",
    "- **IDF (Inverse Document Frequency)** - how exclusive a word is to a single document (words that appear a lot in all documents will have a low **IDF**, filtering out common words like `\"the\"`)\n",
    "- so basically a word that appears many times in the current document (high **TF**) but doesn't appear much in *most* of the *other* documents (high **IDF**) will have a really high **TF-IDF** score\n",
    "    - each unique word in the document could be assigned a **TF-IDF** value, \n",
    "    - you can a matrix out of all the **TF-IDF**'s for each document\n",
    "    - then you could cluster from the matrix with K-Means or something\n",
    "    - you'd assume that documents with similar high-**TF-IDF** words are on similar topics\n",
    "\n",
    "### Equations\n",
    "- $TF\\text{-}IDF$ ([Term Frequency - Inverse Document Frequency](https://www.geeksforgeeks.org/machine-learning/understanding-tf-idf-term-frequency-inverse-document-frequency/))\n",
    "    - $TF\\text{-}IDF(t,d,D) = TF \\times IDF$\n",
    "- $TF$ (Term Frequency)\n",
    "    - $TF(t,d) = \\Large\\frac{\\text{number of times term } t \\text{ appears in document }d}{\\text{total number of terms in document }d}$\n",
    "- $IDF$ (Inverse Document Frequency)\n",
    "    - $IDF(t,D) = \\log\\Large(\\frac{\\text{total number of documents in corpus } D}{\\text{number of documents containing term }t})$\n",
    "\n",
    "| factor | interpretation |\n",
    "| --- | --- |\n",
    "| high $TF$            | term appears frequently in document $d$ |\n",
    "| high $IDF$           | term appears in fewer of documents $D$ |\n",
    "| high $TF\\text{-}IDF$ | term $t$ is frequently used in and unique to document $d$ |\n",
    "\n",
    "### Example - TF-IDF of \"cat\"\n",
    "\n",
    "| \"documents\" $D$ | $TF(t,d)$ | $IDF(t,D)$ | $TF\\text{-}IDF(t,d,D)$ |\n",
    "| --- | --- | --- | --- |\n",
    "| doc $a=$ `The cat sat on the mat.       ` | $1/6$ | $\\log(3/2)\\approx 0.176$ | $1/6\\times 0.176 \\approx 0.029$ |\n",
    "| doc $b=$ `The dog played in the ok park.` | $0/7$ | $\\log(3/2)\\approx 0.176$ | $0/6\\times 0.176 = 0$           |\n",
    "| doc $c=$ `Cats and dogs are great pets. ` | $1/6$ | $\\log(3/2)\\approx 0.176$ | $1/6\\times 0.176 \\approx 0.029$ |\n",
    "\n",
    "### Example - Scikit-Learn\n",
    "- use `sklearn`'s `TfidfVectorizer.fit_transform()` to calculate the TF-IDF's\n",
    "- use very short \"documents\" to keep the table small since there's one TF-IDF per $\\text{number of terms}\\times\\text{number of documents}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "df21d11b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "# create three super-short \"documents\"\n",
    "docs = {\n",
    "    \"d0\": \"Geeks for geeks\",\n",
    "    \"d1\": \"Geeks\",\n",
    "    \"d2\": \"Sweet peas\",\n",
    "}\n",
    "# apply the TF-IDF vectorizer to the \"documents\"\n",
    "vectorizer = TfidfVectorizer()\n",
    "sparse_tfidf_matrix = vectorizer.fit_transform(docs.values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "f34a008f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "documents:\n",
      "{'d0': 'Geeks for geeks', 'd1': 'Geeks', 'd2': 'Sweet peas'}\n",
      "\n",
      "term indexes and IDF's:\n",
      "term      index      idf\n",
      "------  -------  -------\n",
      "for           0  1.69315\n",
      "geeks         1  1.28768\n",
      "peas          2  1.69315\n",
      "sweet         3  1.69315\n",
      "\n",
      "TF-IDF dense  matrix:\n",
      "         for     geeks      peas     sweet\n",
      "--  --------  --------  --------  --------\n",
      "d0  0.549351  0.835592  0         0\n",
      "d1  0         1         0         0\n",
      "d2  0         0         0.707107  0.707107\n",
      "\n",
      "TF-IDF sparse matrix:\n",
      "<Compressed Sparse Row sparse matrix of dtype 'float64'\n",
      "\twith 5 stored elements and shape (3, 4)>\n",
      "  Coords\tValues\n",
      "  (0, 1)\t0.8355915419449176\n",
      "  (0, 0)\t0.5493512310263033\n",
      "  (1, 1)\t1.0\n",
      "  (2, 3)\t0.7071067811865476\n",
      "  (2, 2)\t0.7071067811865476\n",
      "Coords = (doc index, term index),  Values = TF-IDFs\n"
     ]
    }
   ],
   "source": [
    "# make a table with term indices and IDF's\n",
    "from tabulate import tabulate\n",
    "# extract data\n",
    "terms = vectorizer.get_feature_names_out() # each term is a feature name\n",
    "idfs = vectorizer.idf_ # IDF values are in a list (each term has one IDF value)\n",
    "vocab = vectorizer.vocabulary_ # each term gets an index, those are recorded in the vocab dictionary\n",
    "dense_matrix = sparse_tfidf_matrix.toarray() # can convert to a dense matrix (takes up much more space)\n",
    "# combine the term indexes and IDF's into a printable table\n",
    "indx_idfs = [{\"term\": term, \"index\": vocab[term], \"idf\": idf} for term, idf in zip(terms, idfs)]\n",
    "idf_table = tabulate(indx_idfs, headers=\"keys\", tablefmt=\"simple\")\n",
    "# convert the default sparse matrix to a dense matrix and label the rows/columns\n",
    "dense_table = tabulate(dense_matrix, headers=terms, showindex=docs.keys())\n",
    "# Print out the IDF's and TF-IDF's\n",
    "spcr = \"\\n------------------------------------------------------\\n\"\n",
    "print(f\"\\ndocuments:\\n{docs}\")\n",
    "print(f\"\\nterm indexes and IDF's:\\n{idf_table}\")\n",
    "print(f\"\\nTF-IDF dense  matrix:\\n{dense_table}\")\n",
    "print(f\"\\nTF-IDF sparse matrix:\\n{sparse_tfidf_matrix}\\nCoords = (doc index, term index),  Values = TF-IDFs\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f4bc42b",
   "metadata": {},
   "source": [
    "#### TF-IDF Matrix\n",
    "- `dense matrix` lists TF-IDF for every combination of term and document\n",
    "- `sparse matrix` only lists cells with nonzero TF-IDF's by coordinate (saves a lot of space/memory)\n",
    "- most cells of `dense matrix` have `0`'s, i.e. where `TF-IDF` is `0` because that term isn't in that document\n",
    "- can turn the above matrix into this table: \n",
    "\n",
    "| Doc | Term | Coords | tf-idf value |\n",
    "| --- | ---  | --- | --- |\n",
    "| d0 | for   | (0 , 0) | 0.549 |\n",
    "| d0 | geeks | (0 , 1) | 0.835 |\n",
    "| d1 | geeks | (1 , 1) | 1.000 |\n",
    "| d2 | peas  | (2 , 2) | 0.707 |\n",
    "| d2 | sweet | (2 , 3) | 0.707 |\n",
    "\n",
    "So now the documents are vectorized. We've turned documents (lists of strings) into vectors of numbers."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "aiml-notes",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
