{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c47703f4",
   "metadata": {},
   "source": [
    "# Getting Started with NLP\n",
    "- [Getting Started with NLP for Absolute Beginners](https://www.kaggle.com/code/jhoward/getting-started-with-nlp-for-absolute-beginners)\n",
    "- first tutorial in the Kaggle [Natural Language Processing Guide](https://www.kaggle.com/learn-guide/natural-language-processing)\n",
    "\n",
    "## NLP For Classification\n",
    "One of the more useful applications of NLP. Can be used for a bunch of stuff like organizing documents by topic or Sentiment Analysis (finding out if people are saying *positive* or *negative* stuff about your product)\n",
    "\n",
    "## [U.S. Patent Phrase to Phrase Matching Competition](https://www.kaggle.com/c/us-patent-phrase-to-phrase-matching)\n",
    "- compare two words or short phrases\n",
    "    - original competition:\n",
    "        - score them `0`-`1` based on whether they're similar or not\n",
    "        - `0` = totally different meaning, `1` = identical meaning, `0.5` = somewhat similar meaning\n",
    "    - classification version (what we'll do here)\n",
    "        - classify the pairs of words or phrases into `Different`, `Similar`, or `Identical` categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fb9364bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7417b5f",
   "metadata": {},
   "source": [
    "### Get the Dataset\n",
    "- we'll be getting the dataset from Kaggle. \n",
    "    - One problem - when you go to download a data set from a Kaggle competition, you need to agree to the competition rules, including a rule to *not* make the data available to people who haven't agreed to the competition rules. So I can't just add it to my *publicly-available* repo.\n",
    "    - I could just download it from the webpage manually and put it in the right place, but since I can't add it to tracked files, I'd need to re-do that manually for any notebooks that I'd done that for previously anytime I cloned the repo down.\n",
    "- Instead, [install the Kaggle API](https://github.com/Kaggle/kaggle-api/blob/main/docs/README.md) to download the dataset here so I can import it into this notebook, but don't track it in Git.\n",
    "    - If you haven't already, go to the [Competition page](https://www.kaggle.com/c/us-patent-phrase-to-phrase-matching), go to the `Data` tab, and `Accept` the rules of the competition to be allowed to download the dataset.\n",
    "    - If not already installed, install the API (usually with `pip install kaggle`, but since I'm using `UV` as a dependency manager, I used `uv add kaggle`. Running `uv sync` in this repo should install with all the other dependencies)\n",
    "    - On the [Kaggle website](https://www.kaggle.com/), make or login to your account, Click the Profile picture -> `Settings` -> `API` -> `Create new Token` to download `kaggle.json` to computer.\n",
    "        - Move that file to `~/.kaggle/kaggle.json` (`~` is the home directory)\n",
    "        - note: I use Sphinx with `myst_nb` to turn these notebooks into documentation, and `myst_nb` runs the notebooks to check if they still work. Since I can't commit the `kaggle.json` file to the repo without making my private `kaggle api key` publicly available, specify the API key with environment variables instead: `KAGGLE_USERNAME` and `KAGGLE_KEY`. Get those values out of the `kaggle.json` and add them to [GitHub Secrets for the Github Actions Pipeline to use](https://docs.github.com/en/actions/how-tos/write-workflows/choose-what-workflows-do/use-secrets)\n",
    "    - run the cell below to download and unzip the dataset if it doesn't already exist. \n",
    "    - initially this gave me a `\"Forbidden URL\" error` but later it worked. Possibly I hadn't accepted the rules for the competition yet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edde0597",
   "metadata": {},
   "outputs": [],
   "source": [
    "# download and unzip the dataset to this folder if not already downloaded\n",
    "data_dir = Path(\"us-patent-phrase-to-phrase-matching\")\n",
    "if not data_dir.exists():\n",
    "    import kaggle\n",
    "    import zipfile\n",
    "\n",
    "    # download the dataset from Kaggle as zip file\n",
    "    kaggle.api.competition_download_cli(str(data_dir))  \n",
    "    zip_path = data_dir.with_suffix(\".zip\")  # path to the downloaded zip file\n",
    "    zipfile.ZipFile(zip_path).extractall(data_dir)  # unzip the file\n",
    "    zip_path.unlink()  # delete the zip file after unzipping\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e555556",
   "metadata": {},
   "source": [
    "### Examine the DataSet\n",
    "- check the [Competition's Data tab on Kaggle](https://www.kaggle.com/competitions/us-patent-phrase-to-phrase-matching/data) for info on the dataset you couldn't get from the CSV's\n",
    "- `anchor` and `target` phrases are rated for similarity\n",
    "- `context` is the subject of a patent according to the [Cooperative Patent Classification (CPC)](https://en.wikipedia.org/wiki/Cooperative_Patent_Classification)\n",
    "    - `A47`: Section `A` (`Human Necessities`), Class `47` (`Furniture`). ([A47C](https://www.uspto.gov/web/patents/classification/cpc/html/cpc-A47C.html) would be `chairs; sofas; beds`)\n",
    "    - the  phrases `bird` and `Cape Cod` are much closer in the `context` of a `house` than in normal language\n",
    "- `score` rates how similar the `anchor` and `target` phrases are (created by manual expert ratings)\n",
    "    - `0` = not at all similar\n",
    "    - `1` = identical meaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "150d4623",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>anchor</th>\n",
       "      <th>target</th>\n",
       "      <th>context</th>\n",
       "      <th>score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>37d61fd2272659b1</td>\n",
       "      <td>abatement</td>\n",
       "      <td>abatement of pollution</td>\n",
       "      <td>A47</td>\n",
       "      <td>0.50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>7b9652b17b68b7a4</td>\n",
       "      <td>abatement</td>\n",
       "      <td>act of abating</td>\n",
       "      <td>A47</td>\n",
       "      <td>0.75</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>36d72442aefd8232</td>\n",
       "      <td>abatement</td>\n",
       "      <td>active catalyst</td>\n",
       "      <td>A47</td>\n",
       "      <td>0.25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>5296b0c19e1ce60e</td>\n",
       "      <td>abatement</td>\n",
       "      <td>eliminating process</td>\n",
       "      <td>A47</td>\n",
       "      <td>0.50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>54c1e3b9184cb5b6</td>\n",
       "      <td>abatement</td>\n",
       "      <td>forest region</td>\n",
       "      <td>A47</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36468</th>\n",
       "      <td>8e1386cbefd7f245</td>\n",
       "      <td>wood article</td>\n",
       "      <td>wooden article</td>\n",
       "      <td>B44</td>\n",
       "      <td>1.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36469</th>\n",
       "      <td>42d9e032d1cd3242</td>\n",
       "      <td>wood article</td>\n",
       "      <td>wooden box</td>\n",
       "      <td>B44</td>\n",
       "      <td>0.50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36470</th>\n",
       "      <td>208654ccb9e14fa3</td>\n",
       "      <td>wood article</td>\n",
       "      <td>wooden handle</td>\n",
       "      <td>B44</td>\n",
       "      <td>0.50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36471</th>\n",
       "      <td>756ec035e694722b</td>\n",
       "      <td>wood article</td>\n",
       "      <td>wooden material</td>\n",
       "      <td>B44</td>\n",
       "      <td>0.75</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36472</th>\n",
       "      <td>8d135da0b55b8c88</td>\n",
       "      <td>wood article</td>\n",
       "      <td>wooden substrate</td>\n",
       "      <td>B44</td>\n",
       "      <td>0.50</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>36473 rows Ã— 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                     id        anchor                  target context  score\n",
       "0      37d61fd2272659b1     abatement  abatement of pollution     A47   0.50\n",
       "1      7b9652b17b68b7a4     abatement          act of abating     A47   0.75\n",
       "2      36d72442aefd8232     abatement         active catalyst     A47   0.25\n",
       "3      5296b0c19e1ce60e     abatement     eliminating process     A47   0.50\n",
       "4      54c1e3b9184cb5b6     abatement           forest region     A47   0.00\n",
       "...                 ...           ...                     ...     ...    ...\n",
       "36468  8e1386cbefd7f245  wood article          wooden article     B44   1.00\n",
       "36469  42d9e032d1cd3242  wood article              wooden box     B44   0.50\n",
       "36470  208654ccb9e14fa3  wood article           wooden handle     B44   0.50\n",
       "36471  756ec035e694722b  wood article         wooden material     B44   0.75\n",
       "36472  8d135da0b55b8c88  wood article        wooden substrate     B44   0.50\n",
       "\n",
       "[36473 rows x 5 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# import and check the dataset. Looks like it's already scoring similarity of word/phrase pairs.\n",
    "df = pd.read_csv(data_dir / \"train.csv\")\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f18d31b",
   "metadata": {},
   "source": [
    "- using `describe()` reveals 36,473 rows \n",
    "- 733 unique `anchor` phrases, \n",
    "- a whopping 29340 unique `target` phrases, \n",
    "- 106 unique `context`s (subject matter). \n",
    "- Some anchors appear a LOT - the `anchor` `component composite coating` appears 152 times"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0cbe9832",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>anchor</th>\n",
       "      <th>target</th>\n",
       "      <th>context</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>36473</td>\n",
       "      <td>36473</td>\n",
       "      <td>36473</td>\n",
       "      <td>36473</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>unique</th>\n",
       "      <td>36473</td>\n",
       "      <td>733</td>\n",
       "      <td>29340</td>\n",
       "      <td>106</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>top</th>\n",
       "      <td>37d61fd2272659b1</td>\n",
       "      <td>component composite coating</td>\n",
       "      <td>composition</td>\n",
       "      <td>H01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>freq</th>\n",
       "      <td>1</td>\n",
       "      <td>152</td>\n",
       "      <td>24</td>\n",
       "      <td>2186</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                      id                       anchor       target context\n",
       "count              36473                        36473        36473   36473\n",
       "unique             36473                          733        29340     106\n",
       "top     37d61fd2272659b1  component composite coating  composition     H01\n",
       "freq                   1                          152           24    2186"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get descriptive statistics on the object (string) columns\n",
    "df.describe(include=\"object\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d1b9276",
   "metadata": {},
   "source": [
    "### Concatenate the Input\n",
    "- we'll be representing the input to the model like this\n",
    "- `TEXT1: A47; TEXT2: abatement of pollution; ANC1: abatement`\n",
    "- so use `+` to concatenate multiple columns into one \"input\" column \n",
    "- so we'll have one input string per row containing all the important data\n",
    "- I'd forgotten that you can refer to Pandas columns (series's) with dots\n",
    "- i.e. `df['context'] = df.context`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "648ff22d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    TEXT1: A47; TEXT2: abatement of pollution; ANC...\n",
       "1    TEXT1: A47; TEXT2: act of abating; ANC1: abate...\n",
       "2    TEXT1: A47; TEXT2: active catalyst; ANC1: abat...\n",
       "3    TEXT1: A47; TEXT2: eliminating process; ANC1: ...\n",
       "4    TEXT1: A47; TEXT2: forest region; ANC1: abatement\n",
       "Name: input, dtype: object"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# createe an 'input' column by concatenating the important columns with specifiers between\n",
    "df[\"input\"] = \"TEXT1: \" + df.context + \"; TEXT2: \" + df.target + \"; ANC1: \" + df.anchor\n",
    "df.input.head()  # print out the first 5 entries of the new column"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2e769bb",
   "metadata": {},
   "source": [
    "### Tokenize\n",
    "- we're going to pass this to a deep learning model - a `Transformer`\n",
    "    - a neural net expects numbers as inputs, not strings\n",
    "    - must convert these strings to numbers in two steps\n",
    "        - **Tokenization** - split the text into `tokens` (sometimes these are words)\n",
    "        - **Numericalization** - convert each `token` into a number\n",
    "- `Transformers` store their datasets in ... `Dataset` ... objects ...\n",
    "    - take a look at that object after converting to one from Pandas DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b7f50eb2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\gillm\\VisualStudioCode\\aiml-notes\\.venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['id', 'anchor', 'target', 'context', 'score', 'input'],\n",
       "    num_rows: 36473\n",
       "})"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datasets import Dataset, DatasetDict\n",
    "\n",
    "ds = Dataset.from_pandas(df)\n",
    "ds"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6aebb468",
   "metadata": {},
   "source": [
    "- pick an NLP model to start with \n",
    "    (the `tokenization` and `numericalization` methods will depend on your model)\n",
    "- the `microsoft/deberta-v3-small` is a decent starting place for most NLP problems\n",
    "- use `microsoft/deberta-v3-large` for a slower but more accurate model (after initial exploration)\n",
    "- these are pre-trained models, already adept at parsing natural language"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e92abe78",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\gillm\\VisualStudioCode\\aiml-notes\\.venv\\Lib\\site-packages\\transformers\\convert_slow_tokenizer.py:564: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForSequenceClassification, AutoTokenizer\n",
    "\n",
    "# select a model\n",
    "model_nm = \"microsoft/deberta-v3-small\"\n",
    "# model_nm = \"microsoft/deberta-v3-large\"\n",
    "\n",
    "# use AutoTokenizer to create a tokenizer appropriate to the model\n",
    "tokz = AutoTokenizer.from_pretrained(model_nm)\n",
    "#tokz = AutoTokenizer.from_pretrained(model_nm, use_fast=False)  # could use slower tokenizer to prevent a warning about \"byte fallback\" feature from SentencePiece for \"fast\" tokenizer\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "aiml-notes",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
